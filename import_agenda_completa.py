#!/usr/bin/env python3
"""
Script para importar agenda completa no MySQL Hostinger
- Executa UPSERT na tabela agenda_base no MySQL Hostinger
- Compara id_legalone do arquivo com o banco
- Atualiza registros existentes e insere novos (colunas de data tratadas no helper)
- Arquivo: Downloads/import-new-agenda.xlsx
"""

import os
import pandas as pd
import psycopg2
import pyodbc
from dotenv import load_dotenv
from hostinger_mysql_helper import upsert_agenda_base

# Carrega as vari√°veis de ambiente
load_dotenv('config.env')

def read_excel_file(file_path):
    """L√™ um arquivo Excel e retorna um DataFrame do pandas."""
    print(f"üìñ Lendo o arquivo: {file_path}")
    try:
        if file_path.lower().endswith('.xlsx'):
            df = pd.read_excel(file_path)
        elif file_path.lower().endswith('.csv'):
            df = pd.read_csv(file_path)
        else:
            raise ValueError("Formato de arquivo n√£o suportado. Por favor, forne√ßa um arquivo .xlsx ou .csv")
        
        print(f"‚úÖ Arquivo '{file_path}' lido com sucesso.")
        print(f"üìä Total de linhas: {len(df)}")
        print(f"üìã Colunas encontradas: {df.columns.tolist()}")
        return df
    except FileNotFoundError:
        print(f"‚ùå Erro: Arquivo n√£o encontrado em {file_path}")
        return None
    except Exception as e:
        print(f"‚ùå Erro ao ler o arquivo Excel: {e}")
        return None

def extract_date_from_datetime(datetime_str):
    """Extrai a data de uma string no formato dd/mm/aaaa hh:mm:ss ou dd/mm/aaaa hh:mm"""
    if pd.isna(datetime_str) or datetime_str == '':
        return None
    
    try:
        # Tenta primeiro com segundos (formato padr√£o)
        dt = pd.to_datetime(datetime_str, format='%d/%m/%Y %H:%M:%S', errors='coerce')
        if pd.isna(dt):
            # Se falhar, tenta sem segundos (formato prazo_fatal)
            dt = pd.to_datetime(datetime_str, format='%d/%m/%Y %H:%M', errors='coerce')
        if pd.isna(dt):
            return None
        return dt.date()
    except:
        return None

def extract_time_from_datetime(datetime_str):
    """Extrai a hora de uma string no formato dd/mm/aaaa hh:mm:ss"""
    if pd.isna(datetime_str) or datetime_str == '':
        return None
    
    try:
        dt = pd.to_datetime(datetime_str, format='%d/%m/%Y %H:%M:%S', errors='coerce')
        if pd.isna(dt):
            return None
        return dt.time()
    except:
        return None

def generate_link(id_legalone):
    """Gera o link concatenado baseado no id_legalone"""
    if pd.isna(id_legalone):
        return None
    
    base_url = "https://robertomatos.novajus.com.br/agenda/compromissos/DetailsCompromissoTarefa/"
    params = "?hasNavigation=True&currentPage=1&returnUrl=%2Fagenda%2FCompromissoTarefa%2FSearch"
    
    return f"{base_url}{id_legalone}{params}"

def process_excel_file(file_path):
    """Processa o arquivo Excel com todos os tratamentos necess√°rios."""
    print("üîÑ Iniciando processamento do arquivo Excel...")
    
    # 1. Ler o arquivo
    df = read_excel_file(file_path)
    if df is None or df.empty:
        print("‚ùå Erro: N√£o foi poss√≠vel ler o arquivo ou arquivo vazio.")
        return None
    
    print(f"üìä Arquivo lido com sucesso. Linhas: {len(df)}")
    
    try:
        # 2. Criar DataFrame processado com as colunas do Supabase
        df_processed = pd.DataFrame()
        
        # Mapeamento direto (sem tratamento)
        direct_mappings = {
            'id_legalone': 'id_legalone',
            'compromisso_tarefa': 'compromisso_tarefa', 
            'tipo': 'tipo',
            'subtipo': 'subtipo',
            'etiqueta': 'etiqueta',
            'pasta_proc': 'Pasta_proc',
            'numero_cnj': 'numero_cnj',
            'executante': 'executante',
            'executante_sim': 'executante_sim',
            'descricao': 'descricao',
            'status': 'status',
            'cliente-processo': 'cliente-processo'
        }
        
        # Copiar colunas diretas
        for supabase_col, excel_col in direct_mappings.items():
            if excel_col in df.columns:
                df_processed[supabase_col] = df[excel_col]
                print(f"‚úÖ Coluna '{excel_col}' ‚Üí '{supabase_col}'")
            else:
                # Tentar varia√ß√µes do nome da coluna (especialmente para 'cliente-processo')
                if supabase_col == 'cliente-processo':
                    possible_names = ['cliente-processo', 'Cliente-processo', 'Cliente-Processo', 
                                     'CLIENTE-PROCESSO', 'cliente_processo', 'Cliente_processo', 
                                     'Cliente_Processo', 'CLIENTE_PROCESSO']
                    found = False
                    for name in possible_names:
                        if name in df.columns:
                            df_processed[supabase_col] = df[name]
                            print(f"‚úÖ Coluna '{name}' ‚Üí '{supabase_col}'")
                            found = True
                            break
                    if not found:
                        print(f"‚ö†Ô∏è Coluna '{excel_col}' n√£o encontrada no arquivo (tentou varia√ß√µes)")
                        df_processed[supabase_col] = None
                else:
                    print(f"‚ö†Ô∏è Coluna '{excel_col}' n√£o encontrada no arquivo")
                    df_processed[supabase_col] = None
        
        # 3. Tratamento de campos de data/hora
        print("üîÑ Processando campos de data/hora...")
        
        if 'inicio' in df.columns:
            df_processed['inicio_data'] = df['inicio'].apply(extract_date_from_datetime)
            df_processed['inicio_hora'] = df['inicio'].apply(extract_time_from_datetime)
            print("‚úÖ Campo 'inicio' processado ‚Üí 'inicio_data' e 'inicio_hora'")
        
        if 'conclusao_prevista' in df.columns:
            df_processed['conclusao_prevista_data'] = df['conclusao_prevista'].apply(extract_date_from_datetime)
            df_processed['conclusao_prevista_hora'] = df['conclusao_prevista'].apply(extract_time_from_datetime)
            print("‚úÖ Campo 'conclusao_prevista' processado ‚Üí 'conclusao_prevista_data' e 'conclusao_prevista_hora'")
        
        if 'conclusao_efetiva' in df.columns:
            df_processed['conclusao_efetiva_data'] = df['conclusao_efetiva'].apply(extract_date_from_datetime)
            print("‚úÖ Campo 'conclusao_efetiva' processado ‚Üí 'conclusao_efetiva_data'")
        
        if 'cadastro' in df.columns:
            df_processed['cadastro'] = df['cadastro'].apply(extract_date_from_datetime)
            print("‚úÖ Campo 'cadastro' processado ‚Üí formato aaaa/mm/dd")
        
        if 'prazo_fatal' in df.columns:
            df_processed['prazo_fatal_data'] = df['prazo_fatal'].apply(extract_date_from_datetime)
            print("‚úÖ Campo 'prazo_fatal' processado ‚Üí 'prazo_fatal_data'")
        
        # 4. Gerar campo 'link' concatenado
        if 'id_legalone' in df_processed.columns:
            df_processed['link'] = df_processed['id_legalone'].apply(generate_link)
            print("‚úÖ Campo 'link' gerado com sucesso")
        
        # 5. Filtrar apenas linhas onde executante_sim = "Sim"
        print("üîÑ Filtrando linhas onde executante_sim = 'Sim'...")
        if 'executante_sim' in df_processed.columns:
            linhas_antes = len(df_processed)
            df_processed = df_processed[df_processed['executante_sim'] == 'Sim']
            linhas_depois = len(df_processed)
            print(f"‚úÖ Filtro aplicado: {linhas_antes} ‚Üí {linhas_depois} linhas (removidas {linhas_antes - linhas_depois} linhas com 'N√£o')")
        else:
            print("‚ö†Ô∏è Coluna 'executante_sim' n√£o encontrada, pulando filtro")
        
        # 6. Limpar dados nulos e converter tipos
        print("üîÑ Limpando dados e convertendo tipos...")
        
        # Converter id_legalone para int8
        if 'id_legalone' in df_processed.columns:
            df_processed['id_legalone'] = pd.to_numeric(df_processed['id_legalone'], errors='coerce').astype('Int64')
        
        # Converter campos num√©ricos para string (text no Supabase)
        text_columns = ['pasta_proc', 'numero_cnj', 'executante', 'executante_sim', 'descricao', 'link', 'status', 'cliente-processo']
        for col in text_columns:
            if col in df_processed.columns:
                df_processed[col] = df_processed[col].astype(str)
                print(f"‚úÖ Campo '{col}' convertido para string")
        
        # Converter campos de data
        date_columns = ['inicio_data', 'conclusao_prevista_data', 'conclusao_efetiva_data', 'prazo_fatal_data']
        for col in date_columns:
            if col in df_processed.columns:
                df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce').dt.date
        
        # Converter campos de hora
        time_columns = ['inicio_hora', 'conclusao_prevista_hora']
        for col in time_columns:
            if col in df_processed.columns:
                df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce').dt.time
        
        print(f"‚úÖ Processamento conclu√≠do. Linhas processadas: {len(df_processed)}")
        print("üìä Colunas finais:")
        print(df_processed.columns.tolist())
        
        return df_processed
        
    except Exception as e:
        print(f"‚ùå Erro durante o processamento: {e}")
        import traceback
        traceback.print_exc()
        return None

def delete_all_records(table_name):
    """Deleta todos os registros da tabela agenda_base no Supabase"""
    print(f"üóëÔ∏è  Deletando todos os registros da tabela '{table_name}' (Supabase)...")
    
    # Vari√°veis individuais
    user = os.getenv("user") or os.getenv("SUPABASE_USER")
    password = os.getenv("password") or os.getenv("SUPABASE_PASSWORD")
    host = os.getenv("host") or os.getenv("SUPABASE_HOST")
    port = os.getenv("port") or os.getenv("SUPABASE_PORT", "5432")
    dbname = os.getenv("dbname") or os.getenv("SUPABASE_DATABASE")
    
    if not all([user, password, host, dbname]):
        print("‚ùå ERRO: Vari√°veis do Supabase incompletas!")
        return False
    
    try:
        # Conectar usando psycopg2
        conn = psycopg2.connect(
            user=user,
            password=password,
            host=host,
            port=port,
            dbname=dbname,
            sslmode="require"
        )
        
        cursor = conn.cursor()
        
        # Contar registros antes
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_before = cursor.fetchone()[0]
        print(f"üìä Registros existentes antes da exclus√£o: {count_before}")
        
        # Deletar todos os registros
        cursor.execute(f"DELETE FROM {table_name}")
        deleted_count = cursor.rowcount
        
        # Commit das altera√ß√µes
        conn.commit()
        
        # Verificar resultado
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_after = cursor.fetchone()[0]
        
        print(f"‚úÖ Exclus√£o conclu√≠da! {deleted_count} registros deletados")
        print(f"üìä Registros: {count_before} ‚Üí {count_after}")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao deletar registros: {e}")
        import traceback
        traceback.print_exc()
        return False

def delete_all_records_azure(table_name):
    """Deleta todos os registros da tabela agenda_base no Azure SQL Database"""
    print(f"üóëÔ∏è  Deletando todos os registros da tabela '{table_name}' (Azure)...")
    
    conn = None
    try:
        conn = get_azure_connection()
        if not conn:
            print("‚ùå ERRO: N√£o foi poss√≠vel conectar ao Azure SQL Database!")
            return False
        
        cursor = conn.cursor()
        
        # Contar registros antes
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_before = cursor.fetchone()[0]
        print(f"üìä Registros existentes antes da exclus√£o: {count_before}")
        
        # Deletar todos os registros
        cursor.execute(f"DELETE FROM {table_name}")
        deleted_count = cursor.rowcount
        
        # Commit das altera√ß√µes
        conn.commit()
        
        # Verificar resultado
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_after = cursor.fetchone()[0]
        
        print(f"‚úÖ Exclus√£o conclu√≠da! {deleted_count} registros deletados")
        print(f"üìä Registros: {count_before} ‚Üí {count_after}")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao deletar registros no Azure: {e}")
        import traceback
        traceback.print_exc()
        if conn:
            conn.rollback()
            conn.close()
        return False

def insert_all_data(df, table_name):
    """Insere todos os dados processados na tabela agenda_base no Supabase"""
    print(f"üì§ Inserindo {len(df)} registros na tabela '{table_name}' (Supabase)...")
    
    # Vari√°veis individuais
    user = os.getenv("user") or os.getenv("SUPABASE_USER")
    password = os.getenv("password") or os.getenv("SUPABASE_PASSWORD")
    host = os.getenv("host") or os.getenv("SUPABASE_HOST")
    port = os.getenv("port") or os.getenv("SUPABASE_PORT", "5432")
    dbname = os.getenv("dbname") or os.getenv("SUPABASE_DATABASE")
    
    if not all([user, password, host, dbname]):
        print("‚ùå ERRO: Vari√°veis do Supabase incompletas!")
        return False
    
    try:
        # Conectar usando psycopg2
        conn = psycopg2.connect(
            user=user,
            password=password,
            host=host,
            port=port,
            dbname=dbname,
            sslmode="require"
        )
        
        cursor = conn.cursor()
        
        # Preparar dados para inser√ß√£o
        columns_df = df.columns.tolist()
        columns_sql = ", ".join(f'"{col}"' for col in columns_df)
        placeholders = ", ".join(["%s"] * len(columns_df))
        insert_query = f"INSERT INTO {table_name} ({columns_sql}) VALUES ({placeholders})"
        
        print(f"üìä Inserindo {len(df)} registros...")
        
        # Inserir em lotes
        batch_size = 100
        total_inserted = 0
        
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i:i+batch_size]
            print(f"üì¶ Lote {i//batch_size + 1}/{(len(df)-1)//batch_size + 1} ({len(batch_df)} registros)")
            
            for index, row in batch_df.iterrows():
                values = tuple(row.values)
                # Converter NaN para None
                cleaned_values = tuple(None if pd.isna(v) else v for v in values)
                cursor.execute(insert_query, cleaned_values)
                total_inserted += 1
            
            # Commit do lote
            conn.commit()
            print(f"‚úÖ Lote {i//batch_size + 1} inserido com sucesso!")
        
        # Verificar resultado
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_after = cursor.fetchone()[0]
        
        print(f"‚úÖ Inser√ß√£o conclu√≠da! Total inserido: {total_inserted}")
        print(f"üìä Total de registros na tabela: {count_after}")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao inserir dados: {e}")
        import traceback
        traceback.print_exc()
        return False

def insert_all_data_azure(df, table_name):
    """Insere todos os dados processados na tabela agenda_base no Azure SQL Database"""
    print(f"üì§ Inserindo {len(df)} registros na tabela '{table_name}' (Azure)...")
    
    conn = None
    try:
        conn = get_azure_connection()
        if not conn:
            print("‚ùå ERRO: N√£o foi poss√≠vel conectar ao Azure SQL Database!")
            return False
        
        cursor = conn.cursor()
        
        # Obter colunas da tabela (exceto created_at que √© auto)
        cursor.execute(f"""
            SELECT COLUMN_NAME 
            FROM INFORMATION_SCHEMA.COLUMNS 
            WHERE TABLE_NAME = '{table_name}'
            AND COLUMN_NAME != 'created_at'
            ORDER BY ORDINAL_POSITION
        """)
        table_columns = [row[0] for row in cursor.fetchall()]
        
        # Filtrar apenas colunas que existem no DataFrame e na tabela
        columns_df = [col for col in df.columns.tolist() if col in table_columns]
        
        if not columns_df:
            print("‚ùå ERRO: Nenhuma coluna do DataFrame corresponde √†s colunas da tabela!")
            return False
        
        # Preparar dados para inser√ß√£o
        columns_sql = ", ".join(f'[{col}]' for col in columns_df)
        placeholders = ", ".join(["?"] * len(columns_df))
        insert_query = f"INSERT INTO {table_name} ({columns_sql}) VALUES ({placeholders})"
        
        print(f"üìä Inserindo {len(df)} registros...")
        print(f"üìã Colunas a inserir: {columns_df}")
        
        # Inserir em lotes
        batch_size = 100
        total_inserted = 0
        
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i:i+batch_size]
            print(f"üì¶ Lote {i//batch_size + 1}/{(len(df)-1)//batch_size + 1} ({len(batch_df)} registros)")
            
            for index, row in batch_df.iterrows():
                values = []
                for col in columns_df:
                    value = row[col]
                    values.append(None if pd.isna(value) else value)
                
                cursor.execute(insert_query, values)
                total_inserted += 1
            
            # Commit do lote
            conn.commit()
            print(f"‚úÖ Lote {i//batch_size + 1} inserido com sucesso!")
        
        # Verificar resultado
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        count_after = cursor.fetchone()[0]
        
        print(f"‚úÖ Inser√ß√£o conclu√≠da! Total inserido: {total_inserted}")
        print(f"üìä Total de registros na tabela: {count_after}")
        
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        print(f"‚ùå Erro ao inserir dados no Azure: {e}")
        import traceback
        traceback.print_exc()
        if conn:
            conn.rollback()
            conn.close()
        return False

def main():
    """Fun√ß√£o principal"""
    print("="*70)
    print("üöÄ IMPORTAR AGENDA COMPLETA")
    print("="*70)
    
    # Tentar diferentes caminhos poss√≠veis
    possible_paths = [
        "Downloads/import-new-agenda.xlsx",  # Pasta Downloads com mai√∫scula
        "downloads/import-new-agenda.xlsx",  # Pasta downloads com min√∫scula
        os.path.join(os.path.expanduser("~"), "Downloads", "import-new-agenda.xlsx"),  # Downloads do usu√°rio
        "import-new-agenda.xlsx"  # Na raiz do projeto
    ]
    
    file_path = None
    for path in possible_paths:
        if os.path.exists(path):
            file_path = path
            print(f"‚úÖ Arquivo encontrado em: {path}")
            break
    
    # Se n√£o encontrou, pedir ao usu√°rio
    if file_path is None:
        print(f"‚ùå Erro: Arquivo n√£o encontrado em nenhum dos caminhos:")
        for path in possible_paths:
            print(f"   - {path}")
        print("\nüí° Por favor, verifique se o arquivo 'import-new-agenda.xlsx' existe")
        print("   ou forne√ßa o caminho completo do arquivo.")
        return
    
    # 1. Processar arquivo Excel
    print("\n" + "="*70)
    print("üìã ETAPA 1: PROCESSAR ARQUIVO EXCEL")
    print("="*70)
    df_processed = process_excel_file(file_path)
    
    if df_processed is None or df_processed.empty:
        print("‚ùå Erro: N√£o foi poss√≠vel processar o arquivo ou arquivo vazio.")
        return
    
    # 2. Executar UPSERT (MySQL Hostinger)
    print("\n" + "="*70)
    print("üîÑ ETAPA 2: EXECUTAR UPSERT (MYSQL HOSTINGER)")
    print("="*70)
    print("üìã Comparando id_legalone do arquivo com o banco...")
    print("   - Registros existentes ser√£o ATUALIZADOS")
    print("   - Registros novos ser√£o INSERIDOS")
    print(f"üìä Total de registros a processar: {len(df_processed)}")
    
    success_hostinger = upsert_agenda_base(df_processed, "agenda_base", "id_legalone")
    
    if not success_hostinger:
        print("‚ùå Erro: N√£o foi poss√≠vel executar UPSERT no MySQL Hostinger.")
        return
    
    # Conclus√£o
    print("\n" + "="*70)
    print("‚úÖ PROCESSO CONCLU√çDO!")
    print("="*70)
    print(f"üìä Total de registros processados: {len(df_processed)}")
    print("üéâ A tabela agenda_base foi atualizada no MySQL Hostinger usando UPSERT!")

if __name__ == "__main__":
    main()
